import tensorflow as tf # For our Models
import pandas as pd # For reading and processing Data
import seaborn as sns # For Visualization
import keras.layers 
from keras.layers import Normalization, Dense
import matplotlib.pyplot as plt
import keras.losses
from keras.losses import MeanSquaredError, Huber, MeanAbsoluteError
from keras.optimizers import Adam
from keras.metrics import RootMeanSquaredError


# Note: We are still running this in our data preparation file.

# Notice that this time wi will be adding history = model to our function, and compile this version.
# After doing the training we'll be able to recall all this loss values we got during training.

#model.compile(optimizer=Adam(),
#               loss= MeanAbsoluteError()) # Output

#history = model.fit(x,y, #epochs = 100, verbose = 1) # Ouput
#print(history = model.fit(x,y, #epochs = 100, verbose = 1))

#Epoch 1/100
#32/32 [==============================] - 1s 3ms/step - loss: 308520.2188
#Epoch 2/100
#32/32 [==============================] - 0s 4ms/step - loss: 308520.2188
#Epoch 3/100
#32/32 [==============================] - 0s 4ms/step - loss: 308520.1875
#Epoch 4/100
#32/32 [==============================] - 0s 2ms/step - loss: 308520.0938
#Epoch 5/100
#32/32 [==============================] - 0s 2ms/step - loss: 308520.0938
#Epoch 6/100
#32/32 [==============================] - 0s 2ms/step - loss: 308520.0625
#Epoch 7/100
#32/32 [==============================] - 0s 2ms/step - loss: 308520.0312
#Epoch 8/100
#32/32 [==============================] - 0s 2ms/step - loss: 308520.0000
#Epoch 9/100
#32/32 [==============================] - 0s 2ms/step - loss: 308520.0000
#Epoch 10/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.9375
#Epoch 11/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.9375
#Epoch 12/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.8750
#Epoch 13/100
#32/32 [==============================] - 0s 3ms/step - loss: 308519.8125
#Epoch 14/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.8125
#Epoch 15/100
#32/32 [==============================] - 0s 3ms/step - loss: 308519.7500
#Epoch 16/100
#32/32 [==============================] - 0s 3ms/step - loss: 308519.7500
#Epoch 17/100
#32/32 [==============================] - 0s 3ms/step - loss: 308519.6875
#Epoch 18/100
#32/32 [==============================] - 0s 3ms/step - loss: 308519.7188
#Epoch 19/100
#32/32 [==============================] - 0s 3ms/step - loss: 308519.6562
#Epoch 20/100
#32/32 [==============================] - 0s 3ms/step - loss: 308519.5938
#Epoch 21/100
#32/32 [==============================] - 0s 3ms/step - loss: 308519.6250
#Epoch 22/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.5312
#Epoch 23/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.5625
#Epoch 24/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.5000
#Epoch 25/100
#32/32 [==============================] - 0s 4ms/step - loss: 308519.4688
#Epoch 26/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.4062
#Epoch 27/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.3750
#Epoch 28/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.4062
#Epoch 29/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.3125
#Epoch 30/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.3125
#Epoch 31/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.2500
#Epoch 32/100
#32/32 [==============================] - 0s 3ms/step - loss: 308519.2500
#Epoch 33/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.1875
#Epoch 34/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.1875
#Epoch 35/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.1250
#Epoch 36/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.1250
#Epoch 37/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.0625
#Epoch 38/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.0312
#Epoch 39/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.9688
#Epoch 40/100
#32/32 [==============================] - 0s 2ms/step - loss: 308519.0000
#Epoch 41/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.9688
#Epoch 42/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.8438
#Epoch 43/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.8438
#Epoch 44/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.8125
#Epoch 45/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.8125
#Epoch 46/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.7812
#Epoch 47/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.6875
#Epoch 48/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.7500
#Epoch 49/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.7188
#Epoch 50/100
#32/32 [==============================] - 0s 3ms/step - loss: 308518.6875
#Epoch 51/100
#32/32 [==============================] - 0s 4ms/step - loss: 308518.6562
#Epoch 52/100
#32/32 [==============================] - 0s 3ms/step - loss: 308518.5938
#Epoch 53/100
#32/32 [==============================] - 0s 3ms/step - loss: 308518.5938
#Epoch 54/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.5625
#Epoch 55/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.5000
#Epoch 56/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.5312
#Epoch 57/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.4688
#Epoch 58/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.4375
#Epoch 59/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.3750
#Epoch 60/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.3125
#Epoch 61/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.3125
#Epoch 62/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.2500
#Epoch 63/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.2500
#Epoch 64/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.2188
#Epoch 65/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.1875
#Epoch 66/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.1875
#Epoch 67/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.1250
#Epoch 68/100
#32/32 [==============================] - 0s 2ms/step - loss: 308518.1250
#Epoch 69/100
#32/32 [==============================] - 0s 2ms/step - loss: 308517.9688
#Epoch 70/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.9688
#Epoch 71/100
#32/32 [==============================] - 0s 4ms/step - loss: 308517.9688
#Epoch 72/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.9375
#Epoch 73/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.9375
#Epoch 74/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.9062
#Epoch 75/100
#32/32 [==============================] - 0s 4ms/step - loss: 308517.8750
#Epoch 76/100
#32/32 [==============================] - 0s 4ms/step - loss: 308517.8438
#Epoch 77/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.8125
#Epoch 78/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.7500
#Epoch 79/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.7188
#Epoch 80/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.6562
#Epoch 81/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.7500
#Epoch 82/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.6562
#Epoch 83/100
#32/32 [==============================] - 0s 2ms/step - loss: 308517.5938
#Epoch 84/100
#32/32 [==============================] - 0s 4ms/step - loss: 308517.5625
#Epoch 85/100
#32/32 [==============================] - 0s 5ms/step - loss: 308517.5312
#Epoch 86/100
#32/32 [==============================] - 0s 2ms/step - loss: 308517.5000
#Epoch 87/100
#32/32 [==============================] - 0s 2ms/step - loss: 308517.4688
#Epoch 88/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.4375
#Epoch 89/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.4062
#Epoch 90/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.3750
#Epoch 91/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.3125
#Epoch 92/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.2812
#Epoch 93/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.2500
#Epoch 94/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.2500
#Epoch 95/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.1875
#Epoch 96/100
#32/32 [==============================] - 0s 2ms/step - loss: 308517.1250
#Epoch 97/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.1562
#Epoch 98/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.1562
#Epoch 99/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.0938
#Epoch 100/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.0625
#Epoch 1/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.0625
#Epoch 2/100
#32/32 [==============================] - 0s 3ms/step - loss: 308517.0000
#Epoch 3/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.9688
#Epoch 4/100
#32/32 [==============================] - 0s 4ms/step - loss: 308516.9375
#Epoch 5/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.8750
#Epoch 6/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.9062
#Epoch 7/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.8438
#Epoch 8/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.8438
#Epoch 9/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.7812
#Epoch 10/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.7188
#Epoch 11/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.7500
#Epoch 12/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.7188
#Epoch 13/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.5938
#Epoch 14/100
#32/32 [==============================] - 0s 5ms/step - loss: 308516.5625
#Epoch 15/100
#32/32 [==============================] - 0s 5ms/step - loss: 308516.6250
#Epoch 16/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.5000
#Epoch 17/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.5938
#Epoch 18/100
#32/32 [==============================] - 0s 2ms/step - loss: 308516.4688
#Epoch 19/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.4375
#Epoch 20/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.4375
#Epoch 21/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.3750
#Epoch 22/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.3438
#Epoch 23/100
#32/32 [==============================] - 0s 4ms/step - loss: 308516.3125
#Epoch 24/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.2812
#Epoch 25/100
#32/32 [==============================] - 0s 4ms/step - loss: 308516.2812
#Epoch 26/100
#32/32 [==============================] - 0s 4ms/step - loss: 308516.2188
#Epoch 27/100
#32/32 [==============================] - 0s 4ms/step - loss: 308516.1875
#Epoch 28/100
#32/32 [==============================] - 0s 4ms/step - loss: 308516.1250
#Epoch 29/100
#32/32 [==============================] - 0s 3ms/step - loss: 308516.1562
#Epoch 30/100
#32/32 [==============================] - 0s 4ms/step - loss: 308516.0625
#Epoch 31/100
#32/32 [==============================] - 0s 4ms/step - loss: 308516.0625
#Epoch 32/100
#32/32 [==============================] - 0s 6ms/step - loss: 308516.0625
#Epoch 33/100
#32/32 [==============================] - 0s 6ms/step - loss: 308516.0000
#Epoch 34/100
#32/32 [==============================] - 0s 5ms/step - loss: 308515.9688
#Epoch 35/100
#32/32 [==============================] - 0s 4ms/step - loss: 308515.9062
#Epoch 36/100
#32/32 [==============================] - 0s 4ms/step - loss: 308515.9062
#Epoch 37/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.8750
#Epoch 38/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.8125
#Epoch 39/100
#32/32 [==============================] - 0s 2ms/step - loss: 308515.7812
#Epoch 40/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.8125
#Epoch 41/100
#32/32 [==============================] - 0s 4ms/step - loss: 308515.7188
#Epoch 42/100
#32/32 [==============================] - 0s 4ms/step - loss: 308515.6875
#Epoch 43/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.6875
#Epoch 44/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.6875
#Epoch 45/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.6250
#Epoch 46/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.5938
#Epoch 47/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.5625
#Epoch 48/100
#32/32 [==============================] - 0s 4ms/step - loss: 308515.5000
#Epoch 49/100
#32/32 [==============================] - 0s 4ms/step - loss: 308515.5000
#Epoch 50/100
#32/32 [==============================] - 0s 4ms/step - loss: 308515.4688
#Epoch 51/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.4688
#Epoch 52/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.4062
#Epoch 53/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.3125
#Epoch 54/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.3125
#Epoch 55/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.3125
#Epoch 56/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.2188
#Epoch 57/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.2188
#Epoch 58/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.1562
#Epoch 59/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.1250
#Epoch 60/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.1562
#Epoch 61/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.0938
#Epoch 62/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.0625
#Epoch 63/100
#32/32 [==============================] - 0s 2ms/step - loss: 308515.0625
#Epoch 64/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.0312
#Epoch 65/100
#32/32 [==============================] - 0s 3ms/step - loss: 308515.0000
#Epoch 66/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.9375
#Epoch 67/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.8750
#Epoch 68/100
#32/32 [==============================] - 0s 4ms/step - loss: 308514.9062
#Epoch 69/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.9062
#Epoch 70/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.8438
#Epoch 71/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.8125
#Epoch 72/100
#32/32 [==============================] - 0s 4ms/step - loss: 308514.7500
#Epoch 73/100
#32/32 [==============================] - 0s 5ms/step - loss: 308514.7500
#Epoch 74/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.6562
#Epoch 75/100
#32/32 [==============================] - 0s 4ms/step - loss: 308514.6562
#Epoch 76/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.5938
#Epoch 77/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.6250
#Epoch 78/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.5625
#Epoch 79/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.5625
#Epoch 80/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.4688
#Epoch 81/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.4375
#Epoch 82/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.4688
#Epoch 83/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.3750
#Epoch 84/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.4062
#Epoch 85/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.4062
#Epoch 86/100
#32/32 [==============================] - 0s 4ms/step - loss: 308514.3125
#Epoch 87/100
#32/32 [==============================] - 0s 4ms/step - loss: 308514.3125
#Epoch 88/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.2500
#Epoch 89/100
#32/32 [==============================] - 0s 2ms/step - loss: 308514.2188
#Epoch 90/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.2188
#Epoch 91/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.1250
#Epoch 92/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.1250
#Epoch 93/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.0625
#Epoch 94/100
#32/32 [==============================] - 0s 3ms/step - loss: 308514.0625
#Epoch 95/100
#32/32 [==============================] - 0s 2ms/step - loss: 308514.0625
#Epoch 96/100
#32/32 [==============================] - 0s 2ms/step - loss: 308514.0312
#Epoch 97/100
#32/32 [==============================] - 0s 3ms/step - loss: 308513.9688
#Epoch 98/100
#32/32 [==============================] - 0s 3ms/step - loss: 308513.8750
#Epoch 99/100
#32/32 [==============================] - 0s 3ms/step - loss: 308513.9375
#Epoch 100/100
#32/32 [==============================] - 0s 3ms/step - loss: 308513.8750

# Notice that because we added the (history) to our function, we got back all of the loss values from 
#our training.

# After we ran the history = model function, we ran a history.history.
# history.history returned all of our loss values only, no epochs, no steps, and no time.

# Note: We ran ran history.history in our data preparation file.

# history.history # Output
#{'loss': [308520.21875, 308520.1875, 308520.09375, 308520.125, 308520.125, 308520.0, 308520.03125, 308520.0, 
#308519.9375, 308519.90625, 308519.9375, 308519.875, 308519.84375, 308519.84375, 308519.78125, 308519.71875, 
#308519.71875, 308519.6875, 308519.65625, 308519.625, 308519.5625, 308519.5625, 308519.5625, 308519.53125, 
#308519.40625, 308519.5, 308519.40625, 308519.375, 308519.3125, 308519.3125, 308519.25, 308519.1875, 308519.125, 
#308519.1875, 308519.15625, 308519.09375, 308519.0625, 308519.03125, 308519.0, 308518.96875, 308518.96875, 308518.875, 
#308518.90625, 308518.84375, 308518.8125, 308518.8125, 308518.75, 308518.75, 308518.6875, 308518.625, 308518.59375, 
#308518.5625, 308518.5625, 308518.53125, 308518.53125, 308518.46875, 308518.4375, 308518.375, 308518.4375, 308518.34375,
#308518.3125, 308518.3125, 308518.28125, 308518.25, 308518.21875, 308518.125, 308518.125, 308518.125, 308518.03125, 
#308518.03125, 308518.03125, 308518.03125, 308517.90625, 308517.875, 308517.84375, 308517.78125, 308517.78125, 
#308517.78125, 308517.78125, 308517.625, 308517.6875, 308517.65625, 308517.65625, 308517.59375, 308517.5625, 
#308517.46875, 308517.46875, 308517.4375, 308517.4375, 308517.375, 308517.34375, 308517.34375, 308517.25, 308517.25, 
#308517.21875, 308517.21875, 308517.1875, 308517.15625, 308517.15625, 308517.09375]}

# Notice that we are returned all of loss values only, in the form of a list inside of a dictionary.

# Now we will go over using our matplotlib import

# These are the functions that we will be running in our data preparation file.

# plt.plot(history.history["loss"])
# plt.title("model loss")
# plt.ylabel("loss")
# plt.xlabel("epoch")
# plt.legend("train")
# plt.show()


# As usual, results may vary depending on which coding editor we are using.
# Our plot opened up in a separate window.

# Note: Running our plt functions with the original optimizer = Adam() gaves us a return with a small loss
#rate.

# Next we will speed up the learning rate by specifying this in the optimizer = Adam()
# This is what we will change it to. optimizer = Adam(learning_rate = 1)
# See our data preparation file for update.

# Note: After the update, our loss rate increased and it showed in our plt plot that our losses ocurred
#in larger amounts.

#|Y      
#|                   
#|                      Y  = mX + c   
#|             .2  
#|     .1         
#|               
#|              
#|            .3            
#|           
#|       .4  
#|                .5          
#|          .6
#|        
#|     .7
#|                     .8 
#|
#|    
#| 
#|
#|                                                             X
#|------------------------------------------------------------